INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
2017-09-06 20:22:01 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
2017-09-06 20:22:01 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-09-06 20:22:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-06 20:22:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-06 20:22:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
2017-09-06 20:22:01 [scrapy.middleware] INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
INFO: Spider opened
2017-09-06 20:22:01 [scrapy.core.engine] INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-09-06 20:22:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG: Telnet console listening on 127.0.0.1:6032
2017-09-06 20:22:01 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6032
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
2017-09-06 20:22:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
2017-09-06 20:22:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
ERROR: Error processing {'apd_cancel': u'',
 'apd_eff': u'08/04/2017',
 'apd_exp': u'03/26/2005',
 'apd_ext': u'',
 'api_number': u'3003927335',
 'coord_sys': u'NAD83',
 'df_elevation': u'',
 'direction': u'Vertical',
 'gas_cap_plan': u'',
 'gl_elevation': u'6438',
 'init_apd_apprv': u'03/26/2003',
 'kb_elevation': u'',
 'last_insp': u'06/03/2016',
 'last_mit_bht': u'',
 'latitude': u'36.8614883',
 'longitude': u'-107.4326553',
 'mineral_owner': u'Federal',
 'multi_lateral': u'No',
 'mvd': u'3358',
 'operator_name': u'HILCORP ENERGY COMPANY',
 'operator_number': u'372171',
 'pa_date': u'',
 'pa_intent': u'',
 'pbd': u'0',
 'pnr_exp': u'',
 'potash_waiver': u'False',
 'prop_depth': u'3273',
 'prop_form': u'BASIN FRUITLAND COAL',
 'property_name': u'SAN JUAN 31 6 UNIT',
 'property_number': u'318839',
 'shut_in': u'',
 'sing_mult_completion': u'Single',
 'site_release': u'',
 'spud': u'08/04/2003',
 'status': u'Active',
 'surface_location': u'C-35-31N-06W',
 'surface_owner': u'',
 'ta_date': u'',
 'ta_exp': u'',
 'tvd': u'3358',
 'well_id': u'3927335',
 'well_no': u'217A',
 'well_type': u'Gas',
 'work_type': u'New'}
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 34, in process_item
    session.commit()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/session.py", line 906, in commit
    self.transaction.commit()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/session.py", line 461, in commit
    self._prepare_impl()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/session.py", line 441, in _prepare_impl
    self.session.flush()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/session.py", line 2171, in flush
    self._flush(objects)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/session.py", line 2291, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 66, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/session.py", line 2255, in _flush
    flush_context.execute()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py", line 389, in execute
    rec.execute(self)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py", line 548, in execute
    uow
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/persistence.py", line 181, in save_obj
    mapper, table, insert)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/persistence.py", line 835, in _emit_insert_statements
    execute(statement, params)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 945, in execute
    return meth(self, multiparams, params)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/elements.py", line 263, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1053, in _execute_clauseelement
    compiled_sql, distilled_params
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1189, in _execute_context
    context)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1402, in _handle_dbapi_exception
    exc_info
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/compat.py", line 203, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1182, in _execute_context
    context)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/default.py", line 470, in do_execute
    cursor.execute(statement, parameters)
DataError: (psycopg2.DataError) invalid input syntax for type date: ""
LINE 1: ...', '0', '03/26/2003', '08/04/2017', '03/26/2005', '', '', '0...
                                                             ^
 [SQL: 'INSERT INTO import.well_details (well_id, api_number, well_no, property_name, property_number, operator_name, operator_number, status, well_type, work_type, direction, multi_lateral, mineral_owner, surface_owner, surface_location, latitude, longitude, coord_sys, gl_elevation, kb_elevation, df_elevation, sing_mult_completion, potash_waiver, prop_form, prop_depth, tvd, mvd, pbd, init_apd_apprv, apd_eff, apd_exp, apd_cancel, apd_ext, spud, ta_date, shut_in, pa_intent, pa_date, site_release, last_insp, gas_cap_plan, ta_exp, pnr_exp, last_mit_bht) VALUES (%(well_id)s, %(api_number)s, %(well_no)s, %(property_name)s, %(property_number)s, %(operator_name)s, %(operator_number)s, %(status)s, %(well_type)s, %(work_type)s, %(direction)s, %(multi_lateral)s, %(mineral_owner)s, %(surface_owner)s, %(surface_location)s, %(latitude)s, %(longitude)s, %(coord_sys)s, %(gl_elevation)s, %(kb_elevation)s, %(df_elevation)s, %(sing_mult_completion)s, %(potash_waiver)s, %(prop_form)s, %(prop_depth)s, %(tvd)s, %(mvd)s, %(pbd)s, %(init_apd_apprv)s, %(apd_eff)s, %(apd_exp)s, %(apd_cancel)s, %(apd_ext)s, %(spud)s, %(ta_date)s, %(shut_in)s, %(pa_intent)s, %(pa_date)s, %(site_release)s, %(last_insp)s, %(gas_cap_plan)s, %(ta_exp)s, %(pnr_exp)s, %(last_mit_bht)s) RETURNING import.well_details.id'] [parameters: {'apd_cancel': u'', 'api_number': u'3003927335', 'apd_eff': u'08/04/2017', 'potash_waiver': u'False', 'last_mit_bht': u'', 'mineral_owner': u'Federal', 'tvd': u'3358', 'apd_exp': u'03/26/2005', 'property_name': u'SAN JUAN 31 6 UNIT', 'ta_date': u'', 'pa_date': u'', 'operator_number': u'372171', 'apd_ext': u'', 'well_id': u'3927335', 'latitude': u'36.8614883', 'surface_location': u'C-35-31N-06W', 'prop_depth': u'3273', 'status': u'Active', 'direction': u'Vertical', 'shut_in': u'', 'last_insp': u'06/03/2016', 'site_release': u'', 'spud': u'08/04/2003', 'work_type': u'New', 'prop_form': u'BASIN FRUITLAND COAL', 'gl_elevation': u'6438', 'surface_owner': u'', 'gas_cap_plan': u'', 'pnr_exp': u'', 'init_apd_apprv': u'03/26/2003', 'sing_mult_completion': u'Single', 'kb_elevation': u'', 'property_number': u'318839', 'multi_lateral': u'No', 'longitude': u'-107.4326553', 'pbd': u'0', 'df_elevation': u'', 'mvd': u'3358', 'well_type': u'Gas', 'operator_name': u'HILCORP ENERGY COMPANY', 'well_no': u'217A', 'ta_exp': u'', 'coord_sys': u'NAD83', 'pa_intent': u''}]
2017-09-06 20:22:05 [scrapy.core.scraper] ERROR: Error processing {'apd_cancel': u'',
 'apd_eff': u'08/04/2017',
 'apd_exp': u'03/26/2005',
 'apd_ext': u'',
 'api_number': u'3003927335',
 'coord_sys': u'NAD83',
 'df_elevation': u'',
 'direction': u'Vertical',
 'gas_cap_plan': u'',
 'gl_elevation': u'6438',
 'init_apd_apprv': u'03/26/2003',
 'kb_elevation': u'',
 'last_insp': u'06/03/2016',
 'last_mit_bht': u'',
 'latitude': u'36.8614883',
 'longitude': u'-107.4326553',
 'mineral_owner': u'Federal',
 'multi_lateral': u'No',
 'mvd': u'3358',
 'operator_name': u'HILCORP ENERGY COMPANY',
 'operator_number': u'372171',
 'pa_date': u'',
 'pa_intent': u'',
 'pbd': u'0',
 'pnr_exp': u'',
 'potash_waiver': u'False',
 'prop_depth': u'3273',
 'prop_form': u'BASIN FRUITLAND COAL',
 'property_name': u'SAN JUAN 31 6 UNIT',
 'property_number': u'318839',
 'shut_in': u'',
 'sing_mult_completion': u'Single',
 'site_release': u'',
 'spud': u'08/04/2003',
 'status': u'Active',
 'surface_location': u'C-35-31N-06W',
 'surface_owner': u'',
 'ta_date': u'',
 'ta_exp': u'',
 'tvd': u'3358',
 'well_id': u'3927335',
 'well_no': u'217A',
 'well_type': u'Gas',
 'work_type': u'New'}
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 34, in process_item
    session.commit()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/session.py", line 906, in commit
    self.transaction.commit()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/session.py", line 461, in commit
    self._prepare_impl()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/session.py", line 441, in _prepare_impl
    self.session.flush()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/session.py", line 2171, in flush
    self._flush(objects)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/session.py", line 2291, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 66, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/session.py", line 2255, in _flush
    flush_context.execute()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py", line 389, in execute
    rec.execute(self)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py", line 548, in execute
    uow
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/persistence.py", line 181, in save_obj
    mapper, table, insert)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/orm/persistence.py", line 835, in _emit_insert_statements
    execute(statement, params)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 945, in execute
    return meth(self, multiparams, params)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/elements.py", line 263, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1053, in _execute_clauseelement
    compiled_sql, distilled_params
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1189, in _execute_context
    context)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1402, in _handle_dbapi_exception
    exc_info
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/compat.py", line 203, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1182, in _execute_context
    context)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/default.py", line 470, in do_execute
    cursor.execute(statement, parameters)
DataError: (psycopg2.DataError) invalid input syntax for type date: ""
LINE 1: ...', '0', '03/26/2003', '08/04/2017', '03/26/2005', '', '', '0...
                                                             ^
 [SQL: 'INSERT INTO import.well_details (well_id, api_number, well_no, property_name, property_number, operator_name, operator_number, status, well_type, work_type, direction, multi_lateral, mineral_owner, surface_owner, surface_location, latitude, longitude, coord_sys, gl_elevation, kb_elevation, df_elevation, sing_mult_completion, potash_waiver, prop_form, prop_depth, tvd, mvd, pbd, init_apd_apprv, apd_eff, apd_exp, apd_cancel, apd_ext, spud, ta_date, shut_in, pa_intent, pa_date, site_release, last_insp, gas_cap_plan, ta_exp, pnr_exp, last_mit_bht) VALUES (%(well_id)s, %(api_number)s, %(well_no)s, %(property_name)s, %(property_number)s, %(operator_name)s, %(operator_number)s, %(status)s, %(well_type)s, %(work_type)s, %(direction)s, %(multi_lateral)s, %(mineral_owner)s, %(surface_owner)s, %(surface_location)s, %(latitude)s, %(longitude)s, %(coord_sys)s, %(gl_elevation)s, %(kb_elevation)s, %(df_elevation)s, %(sing_mult_completion)s, %(potash_waiver)s, %(prop_form)s, %(prop_depth)s, %(tvd)s, %(mvd)s, %(pbd)s, %(init_apd_apprv)s, %(apd_eff)s, %(apd_exp)s, %(apd_cancel)s, %(apd_ext)s, %(spud)s, %(ta_date)s, %(shut_in)s, %(pa_intent)s, %(pa_date)s, %(site_release)s, %(last_insp)s, %(gas_cap_plan)s, %(ta_exp)s, %(pnr_exp)s, %(last_mit_bht)s) RETURNING import.well_details.id'] [parameters: {'apd_cancel': u'', 'api_number': u'3003927335', 'apd_eff': u'08/04/2017', 'potash_waiver': u'False', 'last_mit_bht': u'', 'mineral_owner': u'Federal', 'tvd': u'3358', 'apd_exp': u'03/26/2005', 'property_name': u'SAN JUAN 31 6 UNIT', 'ta_date': u'', 'pa_date': u'', 'operator_number': u'372171', 'apd_ext': u'', 'well_id': u'3927335', 'latitude': u'36.8614883', 'surface_location': u'C-35-31N-06W', 'prop_depth': u'3273', 'status': u'Active', 'direction': u'Vertical', 'shut_in': u'', 'last_insp': u'06/03/2016', 'site_release': u'', 'spud': u'08/04/2003', 'work_type': u'New', 'prop_form': u'BASIN FRUITLAND COAL', 'gl_elevation': u'6438', 'surface_owner': u'', 'gas_cap_plan': u'', 'pnr_exp': u'', 'init_apd_apprv': u'03/26/2003', 'sing_mult_completion': u'Single', 'kb_elevation': u'', 'property_number': u'318839', 'multi_lateral': u'No', 'longitude': u'-107.4326553', 'pbd': u'0', 'df_elevation': u'', 'mvd': u'3358', 'well_type': u'Gas', 'operator_name': u'HILCORP ENERGY COMPANY', 'well_no': u'217A', 'ta_exp': u'', 'coord_sys': u'NAD83', 'pa_intent': u''}]
INFO: Closing spider (finished)
2017-09-06 20:22:05 [scrapy.core.engine] INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 512,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 257362,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 9, 7, 2, 22, 5, 59183),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'response_received_count': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 9, 7, 2, 22, 1, 339299)}
2017-09-06 20:22:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 512,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 257362,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 9, 7, 2, 22, 5, 59183),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'response_received_count': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 9, 7, 2, 22, 1, 339299)}
INFO: Spider closed (finished)
2017-09-06 20:22:05 [scrapy.core.engine] INFO: Spider closed (finished)
INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
2017-09-06 20:39:51 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
2017-09-06 20:39:51 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-09-06 20:39:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-06 20:39:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-06 20:39:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
2017-09-06 20:39:51 [scrapy.middleware] INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
DEBUG: Telnet console listening on 127.0.0.1:6032
2017-09-06 20:39:51 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6032
INFO: Spider opened
2017-09-06 20:39:51 [scrapy.core.engine] INFO: Spider opened
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
2017-09-06 20:39:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
2017-09-06 20:39:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
DEBUG: Using default logger
2017-09-06 20:39:54 [traitlets] DEBUG: Using default logger
DEBUG: Using default logger
2017-09-06 20:39:54 [traitlets] DEBUG: Using default logger
INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
2017-09-06 20:42:43 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
2017-09-06 20:42:43 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-09-06 20:42:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-06 20:42:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-06 20:42:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
2017-09-06 20:42:43 [scrapy.middleware] INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
DEBUG: Telnet console listening on 127.0.0.1:6033
2017-09-06 20:42:43 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6033
INFO: Spider opened
2017-09-06 20:42:43 [scrapy.core.engine] INFO: Spider opened
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
2017-09-06 20:42:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
2017-09-06 20:42:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
DEBUG: Using default logger
2017-09-06 20:42:46 [traitlets] DEBUG: Using default logger
DEBUG: Using default logger
2017-09-06 20:42:46 [traitlets] DEBUG: Using default logger
INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
2017-09-06 20:44:58 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
2017-09-06 20:44:58 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-09-06 20:44:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-06 20:44:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-06 20:44:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
2017-09-06 20:44:58 [scrapy.middleware] INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
DEBUG: Telnet console listening on 127.0.0.1:6034
2017-09-06 20:44:58 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6034
INFO: Spider opened
2017-09-06 20:44:58 [scrapy.core.engine] INFO: Spider opened
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
2017-09-06 20:44:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
2017-09-06 20:45:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
DEBUG: Using default logger
2017-09-06 20:45:01 [traitlets] DEBUG: Using default logger
DEBUG: Using default logger
2017-09-06 20:45:01 [traitlets] DEBUG: Using default logger
INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
2017-09-06 20:45:42 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
2017-09-06 20:45:42 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-09-06 20:45:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-06 20:45:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-06 20:45:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
2017-09-06 20:45:42 [scrapy.middleware] INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
INFO: Spider opened
2017-09-06 20:45:42 [scrapy.core.engine] INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-09-06 20:45:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG: Telnet console listening on 127.0.0.1:6035
2017-09-06 20:45:42 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6035
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
2017-09-06 20:45:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
2017-09-06 20:45:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
DEBUG: Scraped from <200 https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335>
{'apd_eff': u'08/04/2017',
 'apd_exp': u'03/26/2005',
 'api_number': u'3003927335',
 'coord_sys': u'NAD83',
 'direction': u'Vertical',
 'gl_elevation': u'6438',
 'init_apd_apprv': u'03/26/2003',
 'last_insp': u'06/03/2016',
 'latitude': u'36.8614883',
 'longitude': u'-107.4326553',
 'mineral_owner': u'Federal',
 'multi_lateral': u'No',
 'mvd': u'3358',
 'operator_name': u'HILCORP ENERGY COMPANY',
 'operator_number': u'372171',
 'pbd': u'0',
 'potash_waiver': u'False',
 'prop_depth': u'3273',
 'prop_form': u'BASIN FRUITLAND COAL',
 'property_name': u'SAN JUAN 31 6 UNIT',
 'property_number': u'318839',
 'sing_mult_completion': u'Single',
 'spud': u'08/04/2003',
 'status': u'Active',
 'surface_location': u'C-35-31N-06W',
 'tvd': u'3358',
 'well_id': u'3927335',
 'well_no': u'217A',
 'well_type': u'Gas',
 'work_type': u'New'}
2017-09-06 20:45:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335>
{'apd_eff': u'08/04/2017',
 'apd_exp': u'03/26/2005',
 'api_number': u'3003927335',
 'coord_sys': u'NAD83',
 'direction': u'Vertical',
 'gl_elevation': u'6438',
 'init_apd_apprv': u'03/26/2003',
 'last_insp': u'06/03/2016',
 'latitude': u'36.8614883',
 'longitude': u'-107.4326553',
 'mineral_owner': u'Federal',
 'multi_lateral': u'No',
 'mvd': u'3358',
 'operator_name': u'HILCORP ENERGY COMPANY',
 'operator_number': u'372171',
 'pbd': u'0',
 'potash_waiver': u'False',
 'prop_depth': u'3273',
 'prop_form': u'BASIN FRUITLAND COAL',
 'property_name': u'SAN JUAN 31 6 UNIT',
 'property_number': u'318839',
 'sing_mult_completion': u'Single',
 'spud': u'08/04/2003',
 'status': u'Active',
 'surface_location': u'C-35-31N-06W',
 'tvd': u'3358',
 'well_id': u'3927335',
 'well_no': u'217A',
 'well_type': u'Gas',
 'work_type': u'New'}
INFO: Closing spider (finished)
2017-09-06 20:45:44 [scrapy.core.engine] INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 512,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 257362,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 9, 7, 2, 45, 44, 765818),
 'item_scraped_count': 1,
 'log_count/DEBUG': 4,
 'log_count/INFO': 7,
 'response_received_count': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 9, 7, 2, 45, 42, 621427)}
2017-09-06 20:45:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 512,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 257362,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 9, 7, 2, 45, 44, 765818),
 'item_scraped_count': 1,
 'log_count/DEBUG': 4,
 'log_count/INFO': 7,
 'response_received_count': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 9, 7, 2, 45, 42, 621427)}
INFO: Spider closed (finished)
2017-09-06 20:45:44 [scrapy.core.engine] INFO: Spider closed (finished)
INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
2017-09-06 20:48:31 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
2017-09-06 20:48:31 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-09-06 20:48:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-06 20:48:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-06 20:48:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
2017-09-06 20:48:32 [scrapy.middleware] INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
INFO: Spider opened
2017-09-06 20:48:32 [scrapy.core.engine] INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-09-06 20:48:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG: Telnet console listening on 127.0.0.1:6035
2017-09-06 20:48:32 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6035
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
2017-09-06 20:48:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
2017-09-06 20:48:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
DEBUG: Scraped from <200 https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335>
{'apd_eff': u'08/04/2017',
 'apd_exp': u'03/26/2005',
 'api_number': u'3003927335',
 'coord_sys': u'NAD83',
 'direction': u'Vertical',
 'gl_elevation': u'6438',
 'init_apd_apprv': u'03/26/2003',
 'last_insp': u'06/03/2016',
 'latitude': u'36.8614883',
 'longitude': u'-107.4326553',
 'mineral_owner': u'Federal',
 'multi_lateral': u'No',
 'mvd': u'3358',
 'operator_name': u'HILCORP ENERGY COMPANY',
 'operator_number': u'372171',
 'pbd': u'0',
 'potash_waiver': u'False',
 'prop_depth': u'3273',
 'prop_form': u'BASIN FRUITLAND COAL',
 'property_name': u'SAN JUAN 31 6 UNIT',
 'property_number': u'318839',
 'sing_mult_completion': u'Single',
 'spud': u'08/04/2003',
 'status': u'Active',
 'surface_location': u'C-35-31N-06W',
 'tvd': u'3358',
 'well_id': u'3927335',
 'well_no': u'217A',
 'well_type': u'Gas',
 'work_type': u'New'}
2017-09-06 20:48:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335>
{'apd_eff': u'08/04/2017',
 'apd_exp': u'03/26/2005',
 'api_number': u'3003927335',
 'coord_sys': u'NAD83',
 'direction': u'Vertical',
 'gl_elevation': u'6438',
 'init_apd_apprv': u'03/26/2003',
 'last_insp': u'06/03/2016',
 'latitude': u'36.8614883',
 'longitude': u'-107.4326553',
 'mineral_owner': u'Federal',
 'multi_lateral': u'No',
 'mvd': u'3358',
 'operator_name': u'HILCORP ENERGY COMPANY',
 'operator_number': u'372171',
 'pbd': u'0',
 'potash_waiver': u'False',
 'prop_depth': u'3273',
 'prop_form': u'BASIN FRUITLAND COAL',
 'property_name': u'SAN JUAN 31 6 UNIT',
 'property_number': u'318839',
 'sing_mult_completion': u'Single',
 'spud': u'08/04/2003',
 'status': u'Active',
 'surface_location': u'C-35-31N-06W',
 'tvd': u'3358',
 'well_id': u'3927335',
 'well_no': u'217A',
 'well_type': u'Gas',
 'work_type': u'New'}
INFO: Closing spider (finished)
2017-09-06 20:48:35 [scrapy.core.engine] INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 512,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 257362,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 9, 7, 2, 48, 35, 344280),
 'item_scraped_count': 1,
 'log_count/DEBUG': 4,
 'log_count/INFO': 7,
 'response_received_count': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 9, 7, 2, 48, 32, 50191)}
2017-09-06 20:48:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 512,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 257362,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 9, 7, 2, 48, 35, 344280),
 'item_scraped_count': 1,
 'log_count/DEBUG': 4,
 'log_count/INFO': 7,
 'response_received_count': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 9, 7, 2, 48, 32, 50191)}
INFO: Spider closed (finished)
2017-09-06 20:48:35 [scrapy.core.engine] INFO: Spider closed (finished)
INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
2017-09-06 20:50:52 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
2017-09-06 20:50:52 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-09-06 20:50:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-06 20:50:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-06 20:50:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
2017-09-06 20:50:52 [scrapy.middleware] INFO: Enabled item pipelines:
['new_mexico_scraper.pipelines.NewMexicoScraperPipeline']
INFO: Spider opened
2017-09-06 20:50:52 [scrapy.core.engine] INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-09-06 20:50:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG: Telnet console listening on 127.0.0.1:6035
2017-09-06 20:50:52 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6035
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
2017-09-06 20:50:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/robots.txt> (referer: None)
DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
2017-09-06 20:50:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335> (referer: None)
DEBUG: Scraped from <200 https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335>
{'apd_eff': u'08/04/2017',
 'apd_exp': u'03/26/2005',
 'api_number': u'30-039-27335',
 'coord_sys': u'NAD83',
 'direction': u'Vertical',
 'gl_elevation': u'6438',
 'init_apd_apprv': u'03/26/2003',
 'last_insp': u'06/03/2016',
 'latitude': u'36.8614883',
 'longitude': u'-107.4326553',
 'mineral_owner': u'Federal',
 'multi_lateral': u'No',
 'mvd': u'3358',
 'operator_name': u'HILCORP ENERGY COMPANY',
 'operator_number': u'372171',
 'pbd': u'0',
 'potash_waiver': u'False',
 'prop_depth': u'3273',
 'prop_form': u'BASIN FRUITLAND COAL',
 'property_name': u'SAN JUAN 31 6 UNIT',
 'property_number': u'318839',
 'sing_mult_completion': u'Single',
 'spud': u'08/04/2003',
 'status': u'Active',
 'surface_location': u'C-35-31N-06W',
 'tvd': u'3358',
 'well_id': u'3927335',
 'well_no': u'217A',
 'well_type': u'Gas',
 'work_type': u'New'}
2017-09-06 20:50:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://wwwapps.emnrd.state.nm.us/ocd/ocdpermitting/Data/WellDetails.aspx?api=30-039-27335>
{'apd_eff': u'08/04/2017',
 'apd_exp': u'03/26/2005',
 'api_number': u'30-039-27335',
 'coord_sys': u'NAD83',
 'direction': u'Vertical',
 'gl_elevation': u'6438',
 'init_apd_apprv': u'03/26/2003',
 'last_insp': u'06/03/2016',
 'latitude': u'36.8614883',
 'longitude': u'-107.4326553',
 'mineral_owner': u'Federal',
 'multi_lateral': u'No',
 'mvd': u'3358',
 'operator_name': u'HILCORP ENERGY COMPANY',
 'operator_number': u'372171',
 'pbd': u'0',
 'potash_waiver': u'False',
 'prop_depth': u'3273',
 'prop_form': u'BASIN FRUITLAND COAL',
 'property_name': u'SAN JUAN 31 6 UNIT',
 'property_number': u'318839',
 'sing_mult_completion': u'Single',
 'spud': u'08/04/2003',
 'status': u'Active',
 'surface_location': u'C-35-31N-06W',
 'tvd': u'3358',
 'well_id': u'3927335',
 'well_no': u'217A',
 'well_type': u'Gas',
 'work_type': u'New'}
INFO: Closing spider (finished)
2017-09-06 20:50:55 [scrapy.core.engine] INFO: Closing spider (finished)
INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 512,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 257362,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 9, 7, 2, 50, 55, 875331),
 'item_scraped_count': 1,
 'log_count/DEBUG': 4,
 'log_count/INFO': 7,
 'response_received_count': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 9, 7, 2, 50, 52, 213674)}
2017-09-06 20:50:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 512,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 257362,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 9, 7, 2, 50, 55, 875331),
 'item_scraped_count': 1,
 'log_count/DEBUG': 4,
 'log_count/INFO': 7,
 'response_received_count': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 9, 7, 2, 50, 52, 213674)}
INFO: Spider closed (finished)
2017-09-06 20:50:55 [scrapy.core.engine] INFO: Spider closed (finished)
INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
2017-09-07 16:50:37 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
2017-09-07 16:50:37 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-09-07 16:50:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-07 16:50:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-07 16:50:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
CRITICAL: Unhandled error in Deferred:
2017-09-07 16:50:37 [twisted] CRITICAL: Unhandled error in Deferred:
CRITICAL: Unhandled error in Deferred:
2017-09-07 16:50:37 [twisted] CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 1301, in _inlineCallbacks
    result = g.send(result)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 40, in from_settings
    mw = mwcls()
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 19, in __init__
    create_well_details(engine)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/models.py", line 36, in create_well_details
    DeclarativeBase.metadata.create_all(engine)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 3949, in create_all
    tables=tables)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1929, in _run_visitor
    conn._run_visitor(visitorcallable, element, **kwargs)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1538, in _run_visitor
    **kwargs).traverse_single(element)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py", line 121, in traverse_single
    return meth(obj, **kw)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 712, in visit_metadata
    [t for t in tables if self._can_create_table(t)])
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 1063, in sort_tables_and_constraints
    dependent_on = fkc.referred_table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 2961, in referred_table
    return self.elements[0].column.table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 764, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 1858, in column
    tablekey)
NoReferencedTableError: Foreign key associated with column 'well_details.well_id' could not find table 'wells' with which to generate a foreign key to target column 'id'
2017-09-07 16:50:37 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 1301, in _inlineCallbacks
    result = g.send(result)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 40, in from_settings
    mw = mwcls()
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 19, in __init__
    create_well_details(engine)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/models.py", line 36, in create_well_details
    DeclarativeBase.metadata.create_all(engine)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 3949, in create_all
    tables=tables)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1929, in _run_visitor
    conn._run_visitor(visitorcallable, element, **kwargs)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1538, in _run_visitor
    **kwargs).traverse_single(element)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py", line 121, in traverse_single
    return meth(obj, **kw)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 712, in visit_metadata
    [t for t in tables if self._can_create_table(t)])
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 1063, in sort_tables_and_constraints
    dependent_on = fkc.referred_table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 2961, in referred_table
    return self.elements[0].column.table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 764, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 1858, in column
    tablekey)
NoReferencedTableError: Foreign key associated with column 'well_details.well_id' could not find table 'wells' with which to generate a foreign key to target column 'id'
CRITICAL: 
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 1301, in _inlineCallbacks
    result = g.send(result)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 40, in from_settings
    mw = mwcls()
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 19, in __init__
    create_well_details(engine)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/models.py", line 36, in create_well_details
    DeclarativeBase.metadata.create_all(engine)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 3949, in create_all
    tables=tables)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1929, in _run_visitor
    conn._run_visitor(visitorcallable, element, **kwargs)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1538, in _run_visitor
    **kwargs).traverse_single(element)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py", line 121, in traverse_single
    return meth(obj, **kw)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 712, in visit_metadata
    [t for t in tables if self._can_create_table(t)])
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 1063, in sort_tables_and_constraints
    dependent_on = fkc.referred_table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 2961, in referred_table
    return self.elements[0].column.table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 764, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 1858, in column
    tablekey)
NoReferencedTableError: Foreign key associated with column 'well_details.well_id' could not find table 'wells' with which to generate a foreign key to target column 'id'
2017-09-07 16:50:37 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 1301, in _inlineCallbacks
    result = g.send(result)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 40, in from_settings
    mw = mwcls()
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 19, in __init__
    create_well_details(engine)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/models.py", line 36, in create_well_details
    DeclarativeBase.metadata.create_all(engine)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 3949, in create_all
    tables=tables)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1929, in _run_visitor
    conn._run_visitor(visitorcallable, element, **kwargs)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1538, in _run_visitor
    **kwargs).traverse_single(element)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py", line 121, in traverse_single
    return meth(obj, **kw)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 712, in visit_metadata
    [t for t in tables if self._can_create_table(t)])
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 1063, in sort_tables_and_constraints
    dependent_on = fkc.referred_table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 2961, in referred_table
    return self.elements[0].column.table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 764, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 1858, in column
    tablekey)
NoReferencedTableError: Foreign key associated with column 'well_details.well_id' could not find table 'wells' with which to generate a foreign key to target column 'id'
INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
2017-09-07 16:50:39 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
2017-09-07 16:50:39 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-09-07 16:50:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-07 16:50:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-07 16:50:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
CRITICAL: Unhandled error in Deferred:
2017-09-07 16:50:39 [twisted] CRITICAL: Unhandled error in Deferred:
CRITICAL: Unhandled error in Deferred:
2017-09-07 16:50:39 [twisted] CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 1301, in _inlineCallbacks
    result = g.send(result)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 40, in from_settings
    mw = mwcls()
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 19, in __init__
    create_well_details(engine)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/models.py", line 36, in create_well_details
    DeclarativeBase.metadata.create_all(engine)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 3949, in create_all
    tables=tables)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1929, in _run_visitor
    conn._run_visitor(visitorcallable, element, **kwargs)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1538, in _run_visitor
    **kwargs).traverse_single(element)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py", line 121, in traverse_single
    return meth(obj, **kw)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 712, in visit_metadata
    [t for t in tables if self._can_create_table(t)])
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 1063, in sort_tables_and_constraints
    dependent_on = fkc.referred_table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 2961, in referred_table
    return self.elements[0].column.table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 764, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 1858, in column
    tablekey)
NoReferencedTableError: Foreign key associated with column 'well_details.well_id' could not find table 'wells' with which to generate a foreign key to target column 'id'
2017-09-07 16:50:39 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 1301, in _inlineCallbacks
    result = g.send(result)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 40, in from_settings
    mw = mwcls()
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 19, in __init__
    create_well_details(engine)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/models.py", line 36, in create_well_details
    DeclarativeBase.metadata.create_all(engine)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 3949, in create_all
    tables=tables)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1929, in _run_visitor
    conn._run_visitor(visitorcallable, element, **kwargs)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1538, in _run_visitor
    **kwargs).traverse_single(element)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py", line 121, in traverse_single
    return meth(obj, **kw)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 712, in visit_metadata
    [t for t in tables if self._can_create_table(t)])
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 1063, in sort_tables_and_constraints
    dependent_on = fkc.referred_table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 2961, in referred_table
    return self.elements[0].column.table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 764, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 1858, in column
    tablekey)
NoReferencedTableError: Foreign key associated with column 'well_details.well_id' could not find table 'wells' with which to generate a foreign key to target column 'id'
CRITICAL: 
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 1301, in _inlineCallbacks
    result = g.send(result)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 40, in from_settings
    mw = mwcls()
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 19, in __init__
    create_well_details(engine)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/models.py", line 36, in create_well_details
    DeclarativeBase.metadata.create_all(engine)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 3949, in create_all
    tables=tables)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1929, in _run_visitor
    conn._run_visitor(visitorcallable, element, **kwargs)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1538, in _run_visitor
    **kwargs).traverse_single(element)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py", line 121, in traverse_single
    return meth(obj, **kw)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 712, in visit_metadata
    [t for t in tables if self._can_create_table(t)])
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 1063, in sort_tables_and_constraints
    dependent_on = fkc.referred_table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 2961, in referred_table
    return self.elements[0].column.table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 764, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 1858, in column
    tablekey)
NoReferencedTableError: Foreign key associated with column 'well_details.well_id' could not find table 'wells' with which to generate a foreign key to target column 'id'
2017-09-07 16:50:39 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 1301, in _inlineCallbacks
    result = g.send(result)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 40, in from_settings
    mw = mwcls()
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 19, in __init__
    create_well_details(engine)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/models.py", line 36, in create_well_details
    DeclarativeBase.metadata.create_all(engine)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 3949, in create_all
    tables=tables)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1929, in _run_visitor
    conn._run_visitor(visitorcallable, element, **kwargs)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1538, in _run_visitor
    **kwargs).traverse_single(element)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py", line 121, in traverse_single
    return meth(obj, **kw)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 712, in visit_metadata
    [t for t in tables if self._can_create_table(t)])
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 1063, in sort_tables_and_constraints
    dependent_on = fkc.referred_table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 2961, in referred_table
    return self.elements[0].column.table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 764, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 1858, in column
    tablekey)
NoReferencedTableError: Foreign key associated with column 'well_details.well_id' could not find table 'wells' with which to generate a foreign key to target column 'id'
INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
2017-09-07 16:52:00 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: new_mexico_scraper)
INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
2017-09-07 16:52:00 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'new_mexico_scraper.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['new_mexico_scraper.spiders'], 'BOT_NAME': 'new_mexico_scraper', 'LOG_FILE': 'log.txt', 'DOWNLOAD_DELAY': 2}
INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-09-07 16:52:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-07 16:52:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-07 16:52:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
CRITICAL: Unhandled error in Deferred:
2017-09-07 16:52:00 [twisted] CRITICAL: Unhandled error in Deferred:
CRITICAL: Unhandled error in Deferred:
2017-09-07 16:52:00 [twisted] CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 1301, in _inlineCallbacks
    result = g.send(result)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 40, in from_settings
    mw = mwcls()
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 19, in __init__
    create_well_details(engine)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/models.py", line 36, in create_well_details
    DeclarativeBase.metadata.create_all(engine)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 3949, in create_all
    tables=tables)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1929, in _run_visitor
    conn._run_visitor(visitorcallable, element, **kwargs)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1538, in _run_visitor
    **kwargs).traverse_single(element)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py", line 121, in traverse_single
    return meth(obj, **kw)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 712, in visit_metadata
    [t for t in tables if self._can_create_table(t)])
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 1063, in sort_tables_and_constraints
    dependent_on = fkc.referred_table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 2961, in referred_table
    return self.elements[0].column.table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 764, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 1858, in column
    tablekey)
NoReferencedTableError: Foreign key associated with column 'well_details.well_id' could not find table 'import.wells' with which to generate a foreign key to target column 'id'
2017-09-07 16:52:00 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 1301, in _inlineCallbacks
    result = g.send(result)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 40, in from_settings
    mw = mwcls()
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 19, in __init__
    create_well_details(engine)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/models.py", line 36, in create_well_details
    DeclarativeBase.metadata.create_all(engine)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 3949, in create_all
    tables=tables)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1929, in _run_visitor
    conn._run_visitor(visitorcallable, element, **kwargs)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1538, in _run_visitor
    **kwargs).traverse_single(element)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py", line 121, in traverse_single
    return meth(obj, **kw)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 712, in visit_metadata
    [t for t in tables if self._can_create_table(t)])
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 1063, in sort_tables_and_constraints
    dependent_on = fkc.referred_table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 2961, in referred_table
    return self.elements[0].column.table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 764, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 1858, in column
    tablekey)
NoReferencedTableError: Foreign key associated with column 'well_details.well_id' could not find table 'import.wells' with which to generate a foreign key to target column 'id'
CRITICAL: 
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 1301, in _inlineCallbacks
    result = g.send(result)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 40, in from_settings
    mw = mwcls()
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 19, in __init__
    create_well_details(engine)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/models.py", line 36, in create_well_details
    DeclarativeBase.metadata.create_all(engine)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 3949, in create_all
    tables=tables)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1929, in _run_visitor
    conn._run_visitor(visitorcallable, element, **kwargs)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1538, in _run_visitor
    **kwargs).traverse_single(element)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py", line 121, in traverse_single
    return meth(obj, **kw)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 712, in visit_metadata
    [t for t in tables if self._can_create_table(t)])
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 1063, in sort_tables_and_constraints
    dependent_on = fkc.referred_table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 2961, in referred_table
    return self.elements[0].column.table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 764, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 1858, in column
    tablekey)
NoReferencedTableError: Foreign key associated with column 'well_details.well_id' could not find table 'import.wells' with which to generate a foreign key to target column 'id'
2017-09-07 16:52:00 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py", line 1301, in _inlineCallbacks
    result = g.send(result)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/core/scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py", line 40, in from_settings
    mw = mwcls()
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/pipelines.py", line 19, in __init__
    create_well_details(engine)
  File "/home/postgres/local/psql/new_mexico_scraper/new_mexico_scraper/models.py", line 36, in create_well_details
    DeclarativeBase.metadata.create_all(engine)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 3949, in create_all
    tables=tables)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1929, in _run_visitor
    conn._run_visitor(visitorcallable, element, **kwargs)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/engine/base.py", line 1538, in _run_visitor
    **kwargs).traverse_single(element)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py", line 121, in traverse_single
    return meth(obj, **kw)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 712, in visit_metadata
    [t for t in tables if self._can_create_table(t)])
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/ddl.py", line 1063, in sort_tables_and_constraints
    dependent_on = fkc.referred_table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 2961, in referred_table
    return self.elements[0].column.table
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 764, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/postgres/anaconda2/lib/python2.7/site-packages/sqlalchemy/sql/schema.py", line 1858, in column
    tablekey)
NoReferencedTableError: Foreign key associated with column 'well_details.well_id' could not find table 'import.wells' with which to generate a foreign key to target column 'id'
